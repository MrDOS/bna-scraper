#! /usr/bin/env python3

import argparse
from collections import namedtuple
import json
import logging
import os
import os.path
import re
import requests

DEBUG_PAGES = 'pages.pickle'

BNA_API_URL_PREFIX = 'https://www.britishnewspaperarchive.co.uk/viewer/items/'
BNA_VIEWER_URL_PREFIX = 'https://www.britishnewspaperarchive.co.uk/viewer/'

BNA_URL_PATTERN = re.compile(
    '^https?://(www.)?britishnewspaperarchive.co.uk/viewer(/items)?/(?P<page>(?P<issue>\w+/\d+/\d+)/\d+/(?P<page_number>\d+))'
)

DEFAULT_OUTPUT_FORMAT = '{issue}/{page_number}.jpg'


def url_page_number(url_suffix):
    return url_suffix.rsplit('/', 1)[1]


class Page(namedtuple('Page', ['url_suffix', 'metadata'])):
    def image_url(self):
        for item in self.metadata['Items']:
            for image in item['Images']:
                if url_page_number(image['UriPageOriginal']) == self.page_number():
                    return image['UriPageOriginal']
        return None

    def issue(self):
        return self.metadata['CurrentItemId'].rsplit('/', 1)[0]

    def page_number(self):
        return url_page_number(self.url_suffix)


def main():
    logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s',
                        datefmt='%FT%T%z',
                        level=logging.DEBUG)

    parser = argparse.ArgumentParser(
        description="""
Scrape high-resolution imagery from The British Newspaper Archive. This tool
can retrieve individual pages, ranges of pages, or entire issues.
        """.strip(),
        epilog=f"""
AUTHENTICATION:

As an alternative to passing a browser session token as an argument, you can
set the value of the BNA_SESSION environment variable to the value of your
`session_0` cookie.

PAGE RANGES:

By default, only the single page given will be retrieved. Additional pages from
the same issue can be retrieved by specifying the “-P, --pages” option. Its
value can either be:

- “all”, to cause all pages for the issue to be downloaded.
- A 1-based, unordered, comma-separated list of page numbers and/or end-
  inclusive page ranges. For example, `-P 1,9,3-5` would cause pages 1, 3, 4,
  5, and 9 to be retrieved. Any requested pages not contained in the issue will
  be silently ignored.

OUTPUT FILENAME FORMAT:

By default, page images will be stored in a directory named after the ID of the
issue relative to your working directory when you run the script. For example,

    BL/0002268/18650211/0019.jpg

This can be controlled with the “-o, --output” option. This option takes a
format string describing the paths of saved pages. The format string should be
a path, and the following tokens will be interpolated:

- {{issue}} will be replaced with the ID of the issue that the page comes from.
- {{page_number}} will be replaced with the page number, which the API
  zero-pads to four digits.

Any subdirectories will be created as necessary.

Formally, the default output format is:

   {DEFAULT_OUTPUT_FORMAT}

        """.strip(),
        formatter_class=argparse.RawDescriptionHelpFormatter)
    # parser.add_argument('-u',
    #                     '--username',
    #                     default=os.environ.get('BNA_USERNAME'),
    #                     help='the username for authentication')
    # parser.add_argument('-p',
    #                     '--password',
    #                     default=os.environ.get('BNA_PASSWORD'),
    #                     help='the password for authentication')
    parser.add_argument('-s',
                        '--session',
                        default=os.environ.get('BNA_SESSION'),
                        help='the value of your `session_0` browser cookie')
    parser.add_argument('-P',
                        '--pages',
                        dest='page_ranges',
                        help='the pages to retrieve')
    parser.add_argument('-o',
                        '--output',
                        dest='output_format',
                        default=DEFAULT_OUTPUT_FORMAT,
                        help='the format of output filenames')
    parser.add_argument('-d',
                        '--debug',
                        default=os.environ.get('DEBUG'),
                        action=argparse.BooleanOptionalAction,
                        help='dump API responses for debugging')
    parser.add_argument('url', help='the URL of the publication')

    args = parser.parse_args()

    if args.debug:
        import pickle

    # if args.session is None and (args.username is None
    #                              or args.password is None):
    #     parser.error('Either a session ID or both a username and password ' +
    #                  'must be provided.')

    if args.session is not None:
        logging.info('Using a session ID for authentication.')
    else:
        # TODO: Username/password authentication.
        parser.error('Username/password authentication is unimplemented.')

    session = requests.Session()
    session.cookies.set('session_0', args.session)

    match = BNA_URL_PATTERN.match(args.url)
    if not match:
        logging.error('That doesn\'t look like a BNA URL to me. Giving up!')
        return

    # For debugging/development, it can be handy to dump all the retrieved page
    # metadata out to a file to avoid repeated redownloading.
    loaded_pages = False
    if args.debug:
        try:
            with open(DEBUG_PAGES, 'rb') as f:
                logging.debug('I\'m going to load page data from the file '
                    + f'"{DEBUG_PAGES}" rather than retrieve it from the API. '
                    + 'If you want to re-request the data from the API, '
                    + 'delete this file.')
                pages = pickle.load(f)
                loaded_pages = True
        except FileNotFoundError:
            logging.debug(f'I looked for a file named "{DEBUG_PAGES}" in the '
                + 'hopes of loading page data from it rather than retrieving '
                + "it from the API, but I didn't find one.")

    # Based on some initial page URL, we'll retrieve the metadata for the other
    # pages of the issue.
    if not loaded_pages:
        initial_url_suffix = match.group('page')
        page_filter = build_page_filter(initial_url_suffix, args.page_ranges)
        pages = fetch_pages_metadata(initial_url_suffix, page_filter, session)

        if args.debug:
            with open('pages.pickle', 'wb') as f:
                logging.debug('I\'m going to save page data to the file '
                    + f'"{DEBUG_PAGES}" so that I don\'t need to re-request '
                    + 'it on the next run.')
                pickle.dump(pages, f)

    # Now we can retrieve images as given by the page metadata.
    fetch_pages(pages, args.output_format, session)


def build_page_filter(initial_url_suffix, page_ranges):
    # If no page ranges are given, we'll only grab the initial page.
    if page_ranges is None:
        return lambda pages: [page for page in pages if page.page_number() == url_page_number(initial_url_suffix)]

    # If we're told to grab all pages, we won't filter at all.
    if page_ranges == 'all':
        return lambda pages: pages

    # Otherwise, we'll build a set of permissible page numbers, and check
    # incoming pages against that set.
    page_numbers = set()

    for page_range in page_ranges.split(','):
        page_range = [int(end) for end in page_range.split('-')]

        # Single page number:
        if len(page_range) == 1:
            page_numbers.add(page_range[0])
        # Range of page numbers:
        elif len(page_range) == 2:
            for page_number in range(page_range[0], page_range[1] + 1):
                page_numbers.add(page_number)

    return lambda pages: [page for page in pages if page.page_number() in page_numbers]

def fetch_pages_metadata(initial_url_suffix, page_filter, session):
    """Identify and fetch the metadata for all pages starting from the metadata
    of one single page."""

    # We'll start by making an API request for the page we've been given. The
    # response will tell us about any other pages.
    logging.info('Fetching metadata for the initial page...')
    initial_page = fetch_page_metadata(initial_url_suffix, session)

    # Build out a list of all pages.
    all_pages = [
        Page(page['PageUri'], None)
        if url_page_number(page['PageUri']) != url_page_number(initial_url_suffix) else initial_page
        for page in initial_page.metadata['IssuePages']
    ]

    filtered_pages = []
    for page in page_filter(all_pages):
        # We'll already have data for the initial page.
        if page.metadata is None:
            logging.info(f'Fetching metadata for page {int(page.page_number())}/{len(all_pages)}...')
            page = fetch_page_metadata(page.url_suffix, session)
        else:
            logging.info(f'Page {int(page.page_number())}/{len(all_pages)} ' +
                         'was the initial page. I already have its metadata.')

        filtered_pages.append(page)

    return filtered_pages


def fetch_page_metadata(url_suffix, session):
    """Fetch a page's metadata by its URL suffix."""

    # If the viewer page hasn't been loaded recently (last hour or so), the API
    # will respond with a 500 status code.
    session.get(BNA_VIEWER_URL_PREFIX + url_suffix)
    response = session.get(BNA_API_URL_PREFIX + url_suffix)
    metadata = json.loads(response.text)
    return Page(url_suffix, metadata)


def fetch_pages(pages, output_format, session):
    """Fetch the images for all pages and write them to disk."""

    for page in pages:
        url = page.image_url()

        filename = output_format \
            .replace('{issue}', page.issue()) \
            .replace('{page_number}', page.page_number())
        logging.info('Retrieving %s...', filename)

        dirname = os.path.dirname(filename)
        if dirname != '':
            os.makedirs(dirname, exist_ok=True)

        with open(filename, 'wb') as outfile:
            response = session.get(url, stream=True)
            outfile.write(response.content)


if __name__ == '__main__':
    main()
